import os
import json
import uuid
from pathlib import Path
from pptx import Presentation
import yake

# ==== ì„¤ì • ====
PPT_PATH = "./data/manual_docs/psi_manual.pptx"
CHUNK_SAVE_PATH = "./data/rag_chunks/psi_slide_chunks.json"
LANGUAGE = "ko"  # í•œêµ­ì–´
MAX_KEYWORDS = 5

# í‚¤ì›Œë“œ ì¶”ì¶œê¸° ì´ˆê¸°í™”
kw_extractor = yake.KeywordExtractor(
    lan=LANGUAGE,
    n=1,
    top=MAX_KEYWORDS,
    stopwords=None  # ì˜ì–´ stopword ì œì™¸ ì•ˆ í•¨
)

# í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜
def extract_keywords(text):
    keywords = kw_extractor.extract_keywords(text)
    return [kw for kw, _ in keywords]

# ìŠ¬ë¼ì´ë“œ í…ìŠ¤íŠ¸ â†’ ì²­í¬ë¡œ ë¶„í•  ë° í‚¤ì›Œë“œ ì¶”ì¶œ
def extract_text_chunks(ppt_path):
    prs = Presentation(ppt_path)
    chunks = []

    for idx, slide in enumerate(prs.slides, start=1):
        full_text = ""
        for shape in slide.shapes:
            if hasattr(shape, "text"):
                full_text += shape.text + "\n"

        # ì¤„ ë‹¨ìœ„ ì²­í¬
        for para in full_text.strip().split("\n"):
            clean_text = para.strip()
            if clean_text:
                keywords = extract_keywords(clean_text)
                chunks.append({
                    "id": str(uuid.uuid4()),
                    "slide_index": idx,
                    "text": clean_text,
                    "source": f"slide_{idx}",
                    "keywords": keywords
                })

    return chunks

# ==== ì‹¤í–‰ ====
if __name__ == "__main__":
    print("ğŸ” ìŠ¬ë¼ì´ë“œ í…ìŠ¤íŠ¸ ê¸°ë°˜ RAG ì²­í¬ ìƒì„± ì¤‘...")

    chunks = extract_text_chunks(PPT_PATH)

    os.makedirs(os.path.dirname(CHUNK_SAVE_PATH), exist_ok=True)
    with open(CHUNK_SAVE_PATH, "w", encoding="utf-8") as f:
        json.dump(chunks, f, ensure_ascii=False, indent=2)

    print(f"âœ… ì´ {len(chunks)}ê°œì˜ ì²­í¬ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {CHUNK_SAVE_PATH}")
