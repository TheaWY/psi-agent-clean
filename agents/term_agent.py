import json
import os
from utils.chat_completion import stream_chat_completion
from utils.gpt_wrapper import ask_gpt

# âœ… PSI ìŠ¬ë¼ì´ë“œ ì²­í¬ JSON ë¶ˆëŸ¬ì˜¤ê¸°
PSI_CHUNK_PATH = "data/rag_chunks/psi_slide_chunks.json"
with open(PSI_CHUNK_PATH, "r", encoding="utf-8") as f:
    psi_chunks = json.load(f)

# âœ… GPT ê¸°ë°˜ PSI ìš©ì–´ ì§ˆë¬¸ ì—¬ë¶€ íŒë‹¨
def is_valid_term_question(user_input: str) -> bool:
    prompt = f"""
    ë‹¹ì‹ ì€ 'PSI ì‹œìŠ¤í…œ ìš©ì–´ ì„¤ëª… ì—ì´ì „íŠ¸'ì…ë‹ˆë‹¤.

    ì‚¬ìš©ì ì…ë ¥:
    "{user_input}"

    ì´ ë¬¸ì¥ì´ PSI ì‹œìŠ¤í…œ ê´€ë ¨ ìš©ì–´(Max SR, Main SP, Allocation, SP, SR ë“±)ì— ëŒ€í•œ ì„¤ëª… ìš”ì²­ì´ë©´ 'YES'ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
    ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ 'NO'ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
    """
    return ask_gpt(prompt).strip().upper().startswith("YES")

# âœ… ë‚´ë¶€ ì²­í¬ ê¸°ë°˜ ìš©ì–´ ì„¤ëª…
def find_relevant_chunks(query: str, top_k: int = 1):
    from openai import Embedding

    embedding_response = Embedding.create(
        model="text-embedding-3-small",
        input=query
    )
    query_embedding = embedding_response["data"][0]["embedding"]

    def cosine_similarity(a, b):
        import numpy as np
        a, b = np.array(a), np.array(b)
        return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))

    # âœ… ì‚¬ì „ ì„ë² ë”©ëœ ì²­í¬ê°€ ì—†ê¸° ë•Œë¬¸ì— ìœ ì‚¬ë„ ì¸¡ì • ëŒ€ì‹  í‚¤ì›Œë“œ ê¸°ë°˜ ìœ ì‚¬ë„ ì‚¬ìš© (ê°„ì´ ë°©ì‹)
    results = []
    for chunk in psi_chunks:
        text = chunk["text"]
        if any(k.lower() in text.lower() or k.lower() in query.lower() for k in chunk.get("keywords", [])):
            results.append((text, chunk["keywords"]))

    return results[:top_k] if results else []

# âœ… Term ì—ì´ì „íŠ¸ ë©”ì¸ í•¨ìˆ˜
def explain_term(user_input: str):
    if not is_valid_term_question(user_input):
        return None

    matched_chunks = find_relevant_chunks(user_input)
    if matched_chunks:
        context = "\n---\n".join([c[0] for c in matched_chunks])
        prompt = f"""
        ë„ˆëŠ” PSI ì‹œìŠ¤í…œ ë§¤ë‰´ì–¼ ê¸°ë°˜ ì„¤ëª… ì „ë¬¸ê°€ì•¼.

        ì•„ë˜ëŠ” ê´€ë ¨ ë¬¸ì„œì—ì„œ ê²€ìƒ‰ëœ ë‚´ìš©ì…ë‹ˆë‹¤:
        {context}

        ìœ„ ë‚´ìš©ì„ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ '{user_input}'ì— ëŒ€í•´ ì •í™•í•˜ê³  ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
        """
        return stream_chat_completion(prompt, user_input)

    # ğŸ” fallback: GPT ìì²´ ì§€ì‹ ê¸°ë°˜ ì„¤ëª…
    fallback_prompt = f"""
    ì‚¬ìš©ìê°€ '{user_input}'ë¼ëŠ” ë¬¸ì¥ì—ì„œ ì–´ë–¤ ìš©ì–´ì— ëŒ€í•´ ë¬¼ì–´ë³´ê³  ìˆì–´.
    ì´ ìš©ì–´ëŠ” ë‚´ë¶€ ë§¤ë‰´ì–¼ì—ëŠ” ì—†ì§€ë§Œ, PSI ë§¥ë½ì—ì„œ GPTê°€ ìœ ì¶”í•˜ì—¬ ì„¤ëª…í•´ì¤˜.
    """
    return stream_chat_completion(fallback_prompt, user_input)

# âœ… supervisor_agentì—ì„œ import í•  ìˆ˜ ìˆë„ë¡ alias ì„ ì–¸
term_agent = explain_term
